{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce963525-cfb0-4ce2-a9a4-94445c4d0ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  the key innovation in the transformer architecture is the self attention mechanism which allows the model to weigh the importance of different words\n",
    "#  in a sequence related to each other regardless to their position\n",
    "\n",
    "# KEY COMPONENTS OF TRANSFORMERS\n",
    "\n",
    "# 1) self attention mechanism => it enables the model to focus on diff parts of an input sequence.\n",
    "# 2) positional encoding => since the transformer doesn't inheritly process input in sequences positional encoding are added to represent word order.\n",
    "#  3) multi head attention => multiple attention layers allows the model to focus on different aspects of the input.\n",
    "#  4) fed forward nueral network => a standard fed forward method apllied to each position .\n",
    "#  5) layer normalization => normalize the activation  within the layers to stabilize training.\n",
    "#  6) residual connections => it helps the flow of gradients and combat the vanishing gradient prblm.\n",
    "\n",
    "# mathematical intution of transformer => \n",
    "\n",
    "#  1) self attention => in self attention each word in a sentence interact with every other word. the key intution is to create 3 vectors for each word.\n",
    "#       => query , key , value (3 vectors)\n",
    "#     query (Q) => it will ask how much focus should i give to other words.\n",
    "#     key (k) =>  this provides info. for other words to match against.\n",
    "#     value => this contain the info. of words itself. the attention score of two tokens(word generate how many) is attention =q.k/\n",
    "\n",
    "#    dk (dimension key) = dimension of key vector that is used to scale the dot product to avoid extremely large ,\n",
    "#    the final ourput for a word is the weighted sum of the value vectors where the weights are the attention score.\n",
    "\n",
    "#  2) multi - head attention =>  instead of applying one attention mechanism transformer uses mutliple head attention mechanism.\n",
    "#           each head projects the querys keys and values into different subspaces. \n",
    "#           and learn different expect of the sequences and finally concatenate the output from all heads.\n",
    "#           each attention has its own projection matrix.\n",
    "\n",
    "\n",
    "#  3) positinal encoding => since transformer do not inheretly understand the order of elements like rnn , positional encoding are added to the  input embbedings.\n",
    "\n",
    "#  prob of e => pause stands for position 2i = sin()  ------>\n",
    "\n",
    "#  this added info. about the order of words to the model \n",
    "#  ex => machine translation with transfomers english to french.\n",
    "  # --->  i like deep learning (input)\n",
    "# ans => transformer will first tokenize this sentence and embbed into vectors\n",
    "\n",
    "#  word                      toekn id                 embbeding\n",
    "#   I                          1                      [0.1,0.8,0.7] \n",
    "#   like                       23 \n",
    "#   deep                       45\n",
    "#   learning                    48\n",
    "\n",
    "\n",
    "#  attention of query => this score is used to veda corresponding value vector of deep and aggregated to the compute , the new represented for like this process happens for evry token so each token\n",
    "#   now has a new represntation that is informed by all other tokens in the sequence.\n",
    "#  all this diff attention patterns are combined creating a more comprehensive representation of each word. \n",
    "\n",
    "#  since the word order matters positional encodings are added to the embbedings to introduce the notion of words position in the sequence.\n",
    "#  then the decoder in the trnasformer allow a focus a similar process but add an additional cross layer which attends to the output from the encoder.\n",
    "#  it generated the translated sentence by predicting one token at a time.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
